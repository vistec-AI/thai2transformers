{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename: refactor_span_level_data_collator_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../thai2transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer , DataCollatorForLanguageModeling\n",
    "import math\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from bisect import bisect\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, _collate_batch, tolist\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "\n",
    "import glob, os\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from thai2transformers.datasets import MLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('airesearchth/wangchanberta-base-wiki-20210520-spm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='airesearchth/wangchanberta-base-wiki-20210520-spm', vocab_size=24005, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '‚ñÅ']})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../../dataset/split/thwiki-for-ddp_6.11.2020/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LARGE_DATA_PATH = '../../dataset/split/thwiki-for-ddp_concat_12.11.2020/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏•‡∏•‡πå<_>‡πÄ‡∏õ‡πá‡∏ô‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡∏ä‡∏ô‡∏¥‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏õ‡∏µ‡∏ä‡∏µ‡∏™‡πå‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡∏≠‡∏á‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤<_>(\"E.<_>quagga\")<_>‡∏ä‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á\n",
      "‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏•‡∏•‡πå<_>‡πÄ‡∏õ‡πá‡∏ô‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ‡πÅ‡∏≠‡∏ü‡∏£‡∏¥‡∏Å‡∏≤‡∏ï‡∏≠‡∏ô‡πÉ‡∏ï‡πâ<_>‡πÄ‡∏ä‡πà‡∏ô<_>‡∏ö‡∏≠‡∏ï‡∏™‡∏ß‡∏≤‡∏ô‡∏≤,<_>‡∏™‡∏ß‡∏≤‡∏ã‡∏¥‡πÅ‡∏•‡∏ô‡∏î‡πå,<_>‡πÅ‡∏≠‡∏ü‡∏£‡∏¥‡∏Å‡∏≤‡πÉ‡∏ï‡πâ<_>‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô<_>‡πÄ‡∏õ‡πá‡∏ô‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏™‡∏µ‡∏î‡∏≥‡∏û‡∏≤‡∏î‡∏¢‡∏≤‡∏ß‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ö‡∏•‡∏≤‡∏¢‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏•‡∏á‡πÑ‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏•‡∏≥‡∏ï‡∏±‡∏ß‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡πÉ‡∏ï‡πâ‡∏ó‡πâ‡∏≠‡∏á<_>‡∏°‡∏µ‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏°‡πâ‡∏≤‡∏•‡∏≤‡∏¢‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ\n"
     ]
    }
   ],
   "source": [
    "!head -2 $TRAIN_LARGE_DATA_PATH/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Build features (parallel).\n",
      "\n",
      "[INFO] Start groupping results.\n",
      "[INFO] Done.\n",
      "CPU times: user 316 ms, sys: 89.7 ms, total: 406 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataset = MLMDataset(tokenizer,\n",
    "                           TRAIN_DATA_PATH,\n",
    "                           510)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Build features (parallel).\n",
      "\n",
      "[INFO] Start groupping results.\n",
      "[INFO] Done.\n",
      "CPU times: user 8.76 s, sys: 4.16 s, total: 12.9 s\n",
      "Wall time: 6min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "train_dataset_large = MLMDataset(tokenizer,\n",
    "                           TRAIN_LARGE_DATA_PATH,\n",
    "                           100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token', 'sep_token', 'cls_token', 'pad_token']\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSpanLevelMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for span-level masked language modeling\n",
    "     \n",
    "    adapted from NGramMaskGenerator class\n",
    "    \n",
    "    https://github.com/microsoft/DeBERTa/blob/11fa20141d9700ba2272b38f2d5fce33d981438b/DeBERTa/apps/tasks/mlm_task.py#L36\n",
    "    and\n",
    "    https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    max_gram: int = 3\n",
    "    keep_prob: float = 0.0\n",
    "    mask_prob: float = 1.0\n",
    "    max_preds_per_seq: int = None\n",
    "    max_seq_len: int = 510\n",
    "\n",
    "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15, *args, **kwargs):\n",
    "        super().__init__(tokenizer, mlm=mlm, mlm_probability=mlm_probability)\n",
    "\n",
    "        assert self.mask_prob + self.keep_prob <= 1, \\\n",
    "            f'The prob of using [MASK]({self.mask_prob}) and the prob of using original token({self.keep_prob}) should between [0,1]'\n",
    "\n",
    "        if self.max_preds_per_seq is None:\n",
    "            self.max_preds_per_seq = math.ceil(self.max_seq_len * self.mlm_probability / 10) * 10\n",
    "            self.mask_window = int(1 / self.mlm_probability) # make ngrams per window sized context\n",
    "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        self.vocab_mapping = self.tokenizer.get_vocab()\n",
    "        \n",
    "        self.special_tokens = [self.tokenizer.special_tokens_map[name] for name in  SPECIAL_TOKEN_NAMES]\n",
    "#         print(' self.special_tokens', self.special_tokens)\n",
    "        self.ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "        _pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "        self.pvals = _pvals / _pvals.sum(keepdims=True)\n",
    "        print('max_gram', self.max_gram)\n",
    "    def _choice(self, rng, data, p):\n",
    "        cul = np.cumsum(p)\n",
    "        x = rng.random()*cul[-1]\n",
    "        id = bisect(cul, x)\n",
    "        return data[id]\n",
    "\n",
    "    def _per_token_mask(self, idx, tokens, rng, mask_prob, keep_prob):\n",
    "        label = tokens[idx]\n",
    "        mask = self.tokenizer.mask_token\n",
    "        rand = rng.random()\n",
    "        if rand < mask_prob:\n",
    "            new_label = mask\n",
    "        elif rand < mask_prob + keep_prob:\n",
    "            new_label = label\n",
    "        else:\n",
    "            new_label = rng.choice(self.vocab_words)\n",
    "\n",
    "        tokens[idx] = new_label\n",
    "\n",
    "        return label\n",
    "\n",
    "    def _mask_tokens(self, tokens: List[str], rng=random, **kwargs):\n",
    "\n",
    "        indices = [i for i in range(len(tokens)) if tokens[i] not in self.special_tokens]\n",
    "#         print('debug: indices to be able to be masked', indices)\n",
    "        \n",
    "        unigrams = [ [idx] for idx in indices ]\n",
    "        num_to_predict = min(self.max_preds_per_seq, max(1, int(round(len(tokens) * self.mlm_probability))))\n",
    "           \n",
    "        offset = 0\n",
    "        mask_grams = np.array([False]*len(unigrams))\n",
    "        while offset < len(unigrams):\n",
    "            n = self._choice(rng, self.ngrams, p=self.pvals)\n",
    "            ctx_size = min(n * self.mask_window, len(unigrams)-offset)\n",
    "            m = rng.randint(0, ctx_size-1)\n",
    "            s = offset + m\n",
    "            e = min(offset + m + n, len(unigrams))\n",
    "            offset = max(offset+ctx_size, e)\n",
    "            mask_grams[s:e] = True\n",
    "\n",
    "        target_labels = [None]*len(tokens)\n",
    "        w_cnt = 0\n",
    "        for m,word in zip(mask_grams, unigrams):\n",
    "            if m:\n",
    "                for idx in word:\n",
    "                    label = self._per_token_mask(idx, tokens, rng, self.mask_prob, self.keep_prob)\n",
    "                    target_labels[idx] = label\n",
    "                    w_cnt += 1\n",
    "                if w_cnt >= num_to_predict:\n",
    "                    break\n",
    "\n",
    "        target_labels = [self.vocab_mapping[x] if x else -100 for x in target_labels]\n",
    "        return tokens, target_labels    \n",
    "\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probabilityability`)\n",
    "        # probability_matrix = torch.full(labels.shape, self.mlm_probabilityability)\n",
    "        # if special_tokens_mask is None:\n",
    "        #     special_tokens_mask = [\n",
    "        #         self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        #     ]\n",
    "        #     special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        # else:\n",
    "        #     special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "#         print('inputs', inputs.shape, inputs)\n",
    "        inputs_masked = []\n",
    "        \n",
    "        for i, input in enumerate(inputs):\n",
    "#             print('input',input)\n",
    "            input_tokens = self.tokenizer.convert_ids_to_tokens(input)\n",
    "            \n",
    "\n",
    "            input_masked, _labels = self._mask_tokens(input_tokens)\n",
    "#             print('DEBUG: input_masked', input_masked)\n",
    "            input_masked_ids = self.tokenizer.convert_tokens_to_ids(input_masked)\n",
    "            inputs_masked.append(input_masked_ids)\n",
    "#             print('_labels, ', _labels)\n",
    "#             print('inputs_masked, ', input_masked_ids)\n",
    "            labels.append(_labels)\n",
    "      \n",
    "        return inputs_masked, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SequentialSampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_gram 3\n"
     ]
    }
   ],
   "source": [
    "BZ=32\n",
    "\n",
    "data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              keep_prob=0.0,\n",
    "                                              mask_prob=1.0,\n",
    "                                              max_seq_len=510,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "data_collator_subword_mlm =  DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "data_loader_subword_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator_subword_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         14407 function calls (14404 primitive calls) in 0.030 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.004    0.004    0.008    0.008 data_collator.py:195(_collate_batch)\n",
       "        1    0.004    0.004    0.004    0.004 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "     1320    0.003    0.000    0.003    0.000 tensor.py:468(<lambda>)\n",
       "        8    0.003    0.000    0.008    0.001 tokenization_utils_fast.py:275(convert_ids_to_tokens)\n",
       "        1    0.002    0.002    0.002    0.002 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "     1312    0.002    0.000    0.002    0.000 {method 'id_to_token' of 'tokenizers.Tokenizer' objects}\n",
       "        8    0.002    0.000    0.006    0.001 <ipython-input-8-1161ad0796fc>:63(_mask_tokens)\n",
       "        1    0.002    0.002    0.002    0.002 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method empty}\n",
       "       20    0.001    0.000    0.001    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "      233    0.001    0.000    0.001    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "     1313    0.001    0.000    0.001    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        9    0.000    0.000    0.001    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "      367    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "      233    0.000    0.000    0.002    0.000 <ipython-input-8-1161ad0796fc>:42(_choice)\n",
       "     1313    0.000    0.000    0.001    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "        9    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
       "      233    0.000    0.000    0.001    0.000 random.py:174(randrange)\n",
       "     2648    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "      367    0.000    0.000    0.001    0.000 <ipython-input-8-1161ad0796fc>:48(_per_token_mask)\n",
       "      233    0.000    0.000    0.000    0.000 random.py:224(_randbelow)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-8-1161ad0796fc>:65(<listcomp>)\n",
       "      474    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "      233    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(cumsum)\n",
       "      233    0.000    0.000    0.001    0.000 random.py:218(randint)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-8-1161ad0796fc>:93(<listcomp>)\n",
       "      233    0.000    0.000    0.001    0.000 fromnumeric.py:55(_wrapfunc)\n",
       "      233    0.000    0.000    0.001    0.000 fromnumeric.py:2405(cumsum)\n",
       "        1    0.000    0.000    0.016    0.016 <ipython-input-8-1161ad0796fc>:97(mask_tokens)\n",
       "      233    0.000    0.000    0.000    0.000 {built-in method _bisect.bisect_right}\n",
       "      233    0.000    0.000    0.001    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "      468    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "  750/749    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
       "      242    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      600    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
       "        1    0.000    0.000    0.007    0.007 dataloader.py:320(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 <ipython-input-8-1161ad0796fc>:68(<listcomp>)\n",
       "      233    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        9    0.000    0.000    0.000    0.000 tensor.py:454(__iter__)\n",
       "      233    0.000    0.000    0.000    0.000 fromnumeric.py:2401(_cumsum_dispatcher)\n",
       "      233    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "        1    0.000    0.000    0.030    0.030 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.024    0.024 data_collator.py:339(__call__)\n",
       "        1    0.000    0.000    0.030    0.030 <string>:1(<module>)\n",
       "       19    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "    12/11    0.000    0.000    0.007    0.001 {built-in method builtins.iter}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method builtins.round}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.007    0.007 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        8    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        1    0.000    0.000    0.024    0.024 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.024    0.024 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.024    0.024 fetch.py:42(fetch)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.007    0.007 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        9    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "      2/1    0.000    0.000    0.024    0.024 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:141(__subclasscheck__)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved V2 Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".15*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test lab zone üë©üèª‚Äçüî¨ \n",
    "\n",
    "<br>\n",
    "\n",
    "Idea 3: k-time masking projection\n",
    "\n",
    "\n",
    "``` \n",
    "matrix = [[ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]\n",
    "          [ . . . . . . ]]\n",
    "```\n",
    "\n",
    "Project 1-subword mask\n",
    "\n",
    "Suppose that MLM probability is 15% and the total tokens is 30, then project 1-subword mask 15% (Bernoulli distribution) or 4.5 tokens will be masked.\n",
    "\n",
    "``` \n",
    "matrix = [[ . 1 . . . . ]\n",
    "          [ . . 1 . . . ]\n",
    "          [ . . . . . 1 ]\n",
    "          [ . 1 . . . . ]\n",
    "          [ . . . 1 . . ]]\n",
    "```\n",
    "\n",
    "\n",
    "Project 2-subword mask\n",
    "\n",
    "Suppose that the 2-subword mask have 27.27 % from the masked tokens (from the following distribtuion [0.5455, 0.2727, 0.1818]), 1.3635 tokens will be assigned as 2-subword mask.\n",
    "\n",
    "\n",
    "``` \n",
    "matrix = [[ . 1 . . . . ]\n",
    "          [ . . 2 . . . ]\n",
    "          [ . . . . . 1 ]\n",
    "          [ . 1 . . . . ]\n",
    "          [ . . . 2 . . ]]\n",
    "```\n",
    "\n",
    "Project 3-subword mask\n",
    "\n",
    "Suppose that the 3-subword mask have 18.18 % from the masked tokens (from the following distribtuion [0.5455, 0.2727, 0.1818]), 0.9089 tokens will be assigned as 3-subword mask. This will assign masking only on 1-subword masked tokens. \n",
    "\n",
    "\n",
    "``` \n",
    "matrix = [[ . 1 . . . . ]\n",
    "          [ . . 2 . . . ]\n",
    "          [ . . . . . 1 ]\n",
    "          [ . 3 . . . . ]\n",
    "          [ . . . 2 . . ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "probability_matrix:\n",
      " tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'masked_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6ace9b39f41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nprobability_matrix:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m_masked_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\masked_indices:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbase_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_masked_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masked_indices' is not defined"
     ]
    }
   ],
   "source": [
    "pvals =  [0.5455, 0.2727, 0.1818]\n",
    "\n",
    "mlm_probability = .2\n",
    "probability_matrix = torch.full((5,8), mlm_probability)\n",
    "print('\\nprobability_matrix:\\n', probability_matrix)\n",
    "_masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print('\\masked_indices:\\n', masked_indices)\n",
    "\n",
    "base_indices = (_masked_indices == True).nonzero(as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('base_indices', base_indices)\n",
    "# 2-subword\n",
    "second_probabilty_matrix = torch.full((1, base_indices.shape[0]), pvals[1])\n",
    "print('second_probabilty_matrix', second_probabilty_matrix)\n",
    "\n",
    "second_masked_indices = torch.bernoulli(second_probabilty_matrix).bool()\n",
    "print('second_masked_indices', second_masked_indices)\n",
    "second_indices = (second_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "print('second_indices', second_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('base_indices', base_indices)\n",
    "print('second_indices', second_indices)\n",
    "\n",
    "def filter_indices(base_indices, to_be_filtered_indices):      \n",
    "\n",
    "    to_filter_indices = to_be_filtered_indices[:,1].tolist()\n",
    "    keep_indices = list(set(range(base_indices.shape[0])).difference(set(to_filter_indices)))\n",
    "#     print('\\n  debug:filter_indices:to_filter_indices', to_filter_indices)\n",
    "#     print('\\n  debug:filter_indices:keep_indices', keep_indices)\n",
    "#     base_indices_filtered = [ item for i, item in enumerate(base_indices) if i not in to_filter_indices ]\n",
    "    base_indices_filtered = torch.index_select(base_indices, dim=0, index=torch.LongTensor(keep_indices))\n",
    "\n",
    "#     print('\\n  debug:filter_indices:base_indices_selected', base_indices_selected)\n",
    "\n",
    "    if len(to_filter_indices) == 0:\n",
    "        return base_indices_filtered, None\n",
    "\n",
    "    base_indices_selected = torch.index_select(base_indices, dim=0, index=torch.LongTensor(to_filter_indices))\n",
    "    return base_indices_filtered, base_indices_selected\n",
    "\n",
    "base_indices_filtered, selected_indices = filter_indices(base_indices, second_indices)\n",
    "print('\\n\\nbase_indices_filtered', base_indices_filtered)\n",
    "print('selected_indices', selected_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('base_indices_filtered', base_indices_filtered)\n",
    "\n",
    "third_probabilty_matrix = torch.full((1, len(base_indices_filtered), pvals[2])\n",
    "print('third_probabilty_matrix', third_probabilty_matrix)\n",
    "\n",
    "third_masked_indices = torch.bernoulli(third_probabilty_matrix).bool()\n",
    "print('third_masked_indices', third_masked_indices)\n",
    "third_indices = (third_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "print('third_indices', third_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wrapping up in 1 for loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_indices(base_indices, to_be_filtered_indices):      \n",
    "    indices = to_be_filtered_indices[:,1].tolist()\n",
    "#     print('to be filtered', indices)\n",
    "    base_indices_filtered = [ item for i, item in enumerate(base_indices) if i not in indices ]\n",
    "    base_indices_selected = [ item for i, item in enumerate(base_indices) if i in indices ]\n",
    "    if len(base_indices_selected) == 0:\n",
    "        return torch.stack(base_indices_filtered, dim=0), None\n",
    "    return torch.stack(base_indices_filtered, dim=0), torch.stack(base_indices_selected, dim=0)\n",
    "\n",
    "def mask_tokens(inputs: torch.Tensor,\n",
    "                special_tokens_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    labels = inputs.clone()\n",
    "    labels_to_be_mask = torch.full(inputs.shape, 0.).bool()\n",
    "    pvals =  [0.5455, 0.2727, 0.1818]\n",
    "    K = len(pvals)\n",
    "    mask_indices_by_span_len = [[] for i in range(K)]\n",
    "    \n",
    "    mlm_probability = .2\n",
    "    probability_matrix = torch.full(inputs.shape, mlm_probability)\n",
    "    base_masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    print('\\nprobability_matrix:\\n', masked_indices)\n",
    "    print('\\nbase_masked_indices:\\n', base_masked_indices)\n",
    "    \n",
    "    base_indices = (base_masked_indices == True).nonzero(as_tuple=False)\n",
    "    print('\\nbase_indices:\\n', base_indices)\n",
    "    \n",
    "    _filter_base_indices = base_indices.clone()\n",
    "    for k in range(1, K):\n",
    "       \n",
    "            # 2-subword\n",
    "        print(f'\\n\\n{k+1}-subword masking')\n",
    "        \n",
    "        _probabilty_matrix = torch.full((1, _filter_base_indices.shape[0]), pvals[k])\n",
    "        print(f'\\n{k+1}-subword masking probabilty_matrix', _probabilty_matrix)\n",
    "\n",
    "        _masked_indices = torch.bernoulli(second_probabilty_matrix).bool()\n",
    "        print(f'\\n{k+1}-subword masked_indices', _masked_indices)\n",
    "        to_be_filtered_indices = (_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "        _filter_base_indices, _indices_selected = filter_indices(_filter_base_indices, to_be_filtered_indices)\n",
    "        \n",
    "        print(f'\\n{k+1}-subword _indices_selected: ', _indices_selected)\n",
    "        if _indices_selected == None:\n",
    "            mask_indices_by_span_len[k] = []\n",
    "        else:\n",
    "            mask_indices_by_span_len[k] = _indices_selected\n",
    "\n",
    "    mask_indices_by_span_len[0] = _filter_base_indices\n",
    "    \n",
    "    # Applying label\n",
    "    for k in range(0, K):\n",
    "        mask_indices_by_span_len_at_k = mask_indices_by_span_len[k]\n",
    "        print(f'\\n\\n Perform masking, k={k}')\n",
    "        for indices in mask_indices_by_span_len_at_k:\n",
    "            print('indices', indices)\n",
    "            labels_to_be_mask[indices[0], indices[1]:indices[1]+k+1] = True\n",
    "        \n",
    "        print('labels_to_be_mask', labels_to_be_mask)\n",
    "        print('')\n",
    "    \n",
    "    inputs[labels_to_be_mask] = 24000 # mask token id\n",
    "\n",
    "    labels[~labels_to_be_mask] = -100  # We only compute loss on masked toke\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (torch.rand((8,10))*100).long() \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tokens(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage Zone üó∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "SPECIAL_TOKEN_NAMES = ['bos_token', 'eos_token',\n",
    "                       'sep_token', 'cls_token', 'pad_token']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImprovedV2DataCollatorForSpanLevelMask(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator used for span-level masked language modeling\n",
    "     \n",
    "    adapted from NGramMaskGenerator class\n",
    "    \n",
    "    https://github.com/microsoft/DeBERTa/blob/11fa20141d9700ba2272b38f2d5fce33d981438b/DeBERTa/apps/tasks/mlm_task.py#L36\n",
    "    and\n",
    "    https://github.com/zihangdai/xlnet/blob/0b642d14dd8aec7f1e1ecbf7d6942d5faa6be1f0/data_utils.py\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    max_gram: int = 3\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __new__(cls, tokenizer, mlm, mlm_probability, pad_to_multiple_of, *args, **kwargs):\n",
    "    \n",
    "        obj = object.__new__(cls)\n",
    "        DataCollatorForLanguageModeling.__init__(obj, tokenizer=tokenizer, mlm=mlm,\n",
    "                                                 mlm_probability=mlm_probability,\n",
    "                                                 pad_to_multiple_of=pad_to_multiple_of)\n",
    "        return obj\n",
    "    \n",
    "\n",
    "    def __post_init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
    "        self.vocab_mapping = self.tokenizer.get_vocab()\n",
    "        \n",
    "        self.special_token_ids = [ self.vocab_mapping[self.tokenizer.special_tokens_map[name]] for name in  SPECIAL_TOKEN_NAMES]\n",
    "        self.ngrams = np.arange(1, self.max_gram + 1, dtype=np.int64)\n",
    "        _pvals = 1. / np.arange(1, self.max_gram + 1)\n",
    "        _pvals_np =  _pvals / _pvals.sum(keepdims=True)\n",
    "        self.pvals = torch.Tensor(_pvals / _pvals.sum(keepdims=True))\n",
    "        \n",
    "        pvals_adjusted = [_pvals_np[0]]\n",
    "        for k in range(1, len(_pvals)):\n",
    "            pvals_adjusted.append(_pvals_np[k] / sum(_pvals_np[k:]))\n",
    "\n",
    "        self.pvals_adjusted = torch.Tensor(pvals_adjusted)\n",
    "        print('pvals', self.pvals)\n",
    "        print('pvals_adjusted', self.pvals_adjusted)\n",
    "#         mlm_probability_at_k = []\n",
    "#         remaining_mlm_probability = self.mlm_probability\n",
    "#         print('_pvals', _pvals,     self.pvals)\n",
    "#         for i in range(0, len(_pvals)):\n",
    "#             _mlm_probability = remaining_mlm_probability * self.pvals[i].detach().cpu().numpy()\n",
    "#             mlm_probability_at_k.append(_mlm_probability)\n",
    "#             remaining_mlm_probability -=  _mlm_probability\n",
    "            \n",
    "# #         self.base_mlm_probability = self.mlm_probability + self.mlm_probability * (1/sum(range(1, self.max_gram + 1))/self.max_gram)\n",
    "#         self.base_mlm_probability = sum(mlm_probability_at_k)\n",
    "#         print('base_mlm_probability', self.base_mlm_probability)\n",
    "\n",
    "\n",
    "    def filter_indices(self, base_indices, to_be_filtered_indices):      \n",
    "\n",
    "        to_filter_indices = to_be_filtered_indices[:,1].tolist()\n",
    "        \n",
    "        keep_indices = list(set(range(base_indices.shape[0])).difference(set(to_filter_indices)))\n",
    "\n",
    "        base_indices_filtered = torch.index_select(base_indices, dim=0, index=torch.LongTensor(keep_indices))\n",
    "\n",
    "        if len(to_filter_indices) == 0:\n",
    "            return base_indices_filtered, None\n",
    "        \n",
    "        base_indices_selected = torch.index_select(base_indices, dim=0, index=torch.LongTensor(to_filter_indices))\n",
    "\n",
    "        return base_indices_filtered, base_indices_selected\n",
    "\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor,\n",
    "                    special_tokens_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        labels_to_be_mask = torch.full(inputs.shape, 0., dtype=torch.bool)\n",
    "           \n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = sum(inputs==i for i in self.special_token_ids).bool()\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        K = len(self.pvals)\n",
    "        mask_indices_by_span_len = [[] for i in range(K)]\n",
    "\n",
    "#         probability_matrix = torch.full(inputs.shape, self.base_mlm_probability, dtype=torch.float)\n",
    "        \n",
    "        probability_matrix = torch.full(inputs.shape, self.mlm_probability, dtype=torch.float)\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "\n",
    "        base_masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "        base_indices = (base_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "#         print('self.pvals[k]', self.pvals)\n",
    "        _filter_base_indices = base_indices.clone()\n",
    "        for k in range(1, K):\n",
    "         \n",
    "            _probabilty_matrix = torch.full((1, _filter_base_indices.shape[0]), self.mlm_probability * self.pvals_adjusted[k], dtype=torch.float)\n",
    "\n",
    "            _masked_indices = torch.bernoulli(_probabilty_matrix).bool()\n",
    "\n",
    "            to_be_filtered_indices = (_masked_indices == True).nonzero(as_tuple=False)\n",
    "\n",
    "            _filter_base_indices, _indices_selected = self.filter_indices(_filter_base_indices, to_be_filtered_indices)\n",
    "\n",
    "            if _indices_selected == None:\n",
    "                mask_indices_by_span_len[k] = torch.LongTensor([])\n",
    "            else:\n",
    "                mask_indices_by_span_len[k] = _indices_selected\n",
    "        \n",
    "        mask_indices_by_span_len[0] = _filter_base_indices\n",
    "        # Applying span-level masking\n",
    "        accum_indices = [[],[]]\n",
    "        max_seq_len = inputs.shape[1] - 1\n",
    "\n",
    "        for k in range(0, K):\n",
    "\n",
    "            list_of_indices = mask_indices_by_span_len[k]\n",
    "            \n",
    "            if list_of_indices.shape == (0,):\n",
    "                continue\n",
    "            else:\n",
    "                for j in range(k+1):\n",
    "                    max_indices = torch.full((list_of_indices.shape[0],), max_seq_len, dtype=torch.long)\n",
    "                    left, right = (list_of_indices[:, 0], \\\n",
    "                                   torch.min(list_of_indices[:, 1] + j, max_indices))\n",
    "\n",
    "                    accum_indices[0].append(left)\n",
    "                    accum_indices[1].append(right)\n",
    "        accum_indices[0] = list(filter(lambda x: x.shape != (0,), accum_indices[0]))\n",
    "        accum_indices[1] = list(filter(lambda x: x.shape != (0,), accum_indices[1]))\n",
    "        \n",
    "        if len(accum_indices[0]) != 0: \n",
    "            accum_indices_flatten  = (torch.cat(accum_indices[0]), torch.cat(accum_indices[1]))\n",
    "            labels_to_be_mask.index_put_(accum_indices_flatten, torch.tensor([1.]).bool())\n",
    "            labels_to_be_mask.masked_fill_(special_tokens_mask, value=0.0).bool()\n",
    "\n",
    "        inputs[labels_to_be_mask] = self.tokenizer.mask_token_id\n",
    "        labels[~labels_to_be_mask] = -100  # We only compute loss on masked token\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 0.081825\n",
      "b2 0.0185913225\n",
      "b3 0.0090143125695\n",
      "\n",
      "sum: 0.10943063506949999\n"
     ]
    }
   ],
   "source": [
    "pvals=[0.5455, 0.2727, 0.1818]\n",
    "base_prob =.15\n",
    "\n",
    "b1 = base_prob * pvals[0]\n",
    "print('b1', b1)\n",
    "b2 = (base_prob - (b1)) * (pvals[1])\n",
    "print('b2', b2)\n",
    "b3 = (base_prob - (b1) - b2) * (pvals[2])\n",
    "print('b3', b3)\n",
    "# b4 = (base_prob - (b1) - b2- b3) * (pvals[3])\n",
    "# print('b4', b4)\n",
    "print('\\nsum:', sum([b1,b2,b3]))\n",
    "\n",
    "# print(b1 )\n",
    "# print(b2 )\n",
    "# print(b3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15833333333333333"
      ]
     },
     "execution_count": 1323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prob + (base_prob * 1 / (sum(range(1, 3+1))) / 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvals tensor([0.5455, 0.2727, 0.1818])\n",
      "pvals_adjusted tensor([0.5455, 0.6000, 1.0000])\n",
      "pvals tensor([0.5455, 0.2727, 0.1818])\n",
      "pvals_adjusted tensor([0.5455, 0.6000, 1.0000])\n",
      "text 1: ‚ñÅ|‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß|‡∏ï‡∏≤‡∏°|‚ñÅ|‡∏´‡∏£‡∏∑‡∏≠|‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå|‡∏ß‡πà‡∏≤|‚ñÅ|‡∏≠|‡∏≤|‡∏ü|‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏ä‡πá‡∏≠‡∏Å|‚ñÅ|(|‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©|:|‚ñÅ|af|ter|sh|ock|)|‚ñÅ|‡πÄ‡∏õ‡πá‡∏ô|‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß|‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å|‡∏ó‡∏µ‡πà|‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô|‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å|‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß|‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà|‡∏ó‡∏µ‡πà‡∏°‡∏µ|‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 34\n",
      "\n",
      "text 2: ‚ñÅ|‡∏≠|‡∏≤|‡∏ü|‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏¢‡∏π|‚ñÅ|(|After|‚ñÅ|You|‚ñÅ|Des|s|ert|‚ñÅ|Ca|f|√©|)|‚ñÅ|‡πÄ‡∏õ‡πá‡∏ô|‡∏£‡πâ‡∏≤‡∏ô|‡∏Ç‡∏ô‡∏°‡∏´‡∏ß‡∏≤‡∏ô|‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥|‡∏•‡∏∞|‡∏°‡∏∏‡∏ô|‡∏ó‡∏µ‡πà|‡∏Ñ‡∏£‡∏≠‡∏á|‡πÉ‡∏à|‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤|‡πÄ‡∏õ‡πá‡∏ô|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ|‚ñÅ|‡πÇ‡∏î‡∏¢‡∏°‡∏µ|‡πÄ‡∏°‡∏ô‡∏π|‡∏Ç‡∏≠‡∏á|‡∏´‡∏ß‡∏≤‡∏ô|‡∏≠‡∏£‡πà‡∏≠‡∏¢|‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å|‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢|‡πÄ‡∏°‡∏ô‡∏π|‚ñÅ 43\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "res {'input_ids': tensor([[    5,     8, 24004, 24004,     8,    28,  9962,    38,     8,    52,\n",
      "             9,   265,   712, 13225,     8, 24004,   236,    94,     8, 14051,\n",
      "          2414,  2711, 11661,    18,     8,    14,  2827,   897,    12,   319,\n",
      "           134,  2827,   604,    93,   862,     6,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    5,     8,    52, 24004, 24004, 24004,   406,     8, 24004, 23639,\n",
      "             8,  3918,     8, 14929,    70, 13666,     8,  6101, 24004,  2912,\n",
      "            18, 24004,    14,  1665, 20046,  6886,   674,  5659,    12,  2226,\n",
      "         24004,  3695,    14,  1987,     8,   198, 14237,    17,  2858, 24004,\n",
      "         12010,   971, 14237,     8,     6]]), 'labels': tensor([[ -100,  -100,  2827,    83,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,    15,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,     9,   265,   712,  -100,  -100,    15,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   737,  -100,\n",
      "          -100,     8,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           323,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 16595,\n",
      "          -100,  -100,  -100,  -100,  -100]])}\n",
      "\n",
      "inputs 1 (after):\n",
      " <s>|‚ñÅ|<mask>|<mask>|‚ñÅ|‡∏´‡∏£‡∏∑‡∏≠|‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå|‡∏ß‡πà‡∏≤|‚ñÅ|‡∏≠|‡∏≤|‡∏ü|‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏ä‡πá‡∏≠‡∏Å|‚ñÅ|<mask>|‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©|:|‚ñÅ|af|ter|sh|ock|)|‚ñÅ|‡πÄ‡∏õ‡πá‡∏ô|‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß|‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å|‡∏ó‡∏µ‡πà|‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô|‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å|‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß|‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà|‡∏ó‡∏µ‡πà‡∏°‡∏µ|‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤|</s>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>|<pad>\n",
      "\n",
      "\n",
      "inputs 2 (after):\n",
      " <s>|‚ñÅ|‡∏≠|<mask>|<mask>|<mask>|‡∏¢‡∏π|‚ñÅ|<mask>|After|‚ñÅ|You|‚ñÅ|Des|s|ert|‚ñÅ|Ca|<mask>|√©|)|<mask>|‡πÄ‡∏õ‡πá‡∏ô|‡∏£‡πâ‡∏≤‡∏ô|‡∏Ç‡∏ô‡∏°‡∏´‡∏ß‡∏≤‡∏ô|‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥|‡∏•‡∏∞|‡∏°‡∏∏‡∏ô|‡∏ó‡∏µ‡πà|‡∏Ñ‡∏£‡∏≠‡∏á|<mask>|‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤|‡πÄ‡∏õ‡πá‡∏ô|‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ|‚ñÅ|‡πÇ‡∏î‡∏¢‡∏°‡∏µ|‡πÄ‡∏°‡∏ô‡∏π|‡∏Ç‡∏≠‡∏á|‡∏´‡∏ß‡∏≤‡∏ô|<mask>|‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å|‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢|‡πÄ‡∏°‡∏ô‡∏π|‚ñÅ|</s>\n"
     ]
    }
   ],
   "source": [
    "imp_data_collator_span_mlm =  ImprovedV2DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=1)\n",
    "\n",
    "\n",
    "text1 = \"\"\"‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß‡∏ï‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ß‡πà‡∏≤ ‡∏≠‡∏≤‡∏ü‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πá‡∏≠‡∏Å (‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: aftershock) ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\"\"\"\n",
    "text2 = \"\"\"‡∏≠‡∏≤‡∏ü‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏¢‡∏π (After You Dessert Caf√©) ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡∏ô‡∏°‡∏´‡∏ß‡∏≤‡∏ô‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏•‡∏∞‡∏°‡∏∏‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏á‡πÉ‡∏à‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡πÄ‡∏°‡∏ô‡∏π‡∏Ç‡∏≠‡∏á‡∏´‡∏ß‡∏≤‡∏ô‡∏≠‡∏£‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏°‡∏ô‡∏π \"\"\"\n",
    "tokens = tokenizer.tokenize(text1)\n",
    "print('text 1:', '|'.join(tokens), len(tokens))\n",
    "tokens = tokenizer.tokenize(text2)\n",
    "print('\\ntext 2:','|'.join(tokens), len(tokens))\n",
    "\n",
    "\n",
    "inputs_1 = tokenizer.encode_plus(text1, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "inputs_2 = tokenizer.encode_plus(text2, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "\n",
    "res = imp_data_collator_span_mlm((inputs_1, inputs_2))\n",
    "print('\\n\\n')\n",
    "# print('inputs 1 (before)', inputs_1)\n",
    "# print('inputs 2 (before)', inputs_2)\n",
    "print('\\n\\n\\nres', res)\n",
    "\n",
    "print('\\ninputs 1 (after):\\n', '|'.join(tokenizer.convert_ids_to_tokens(res['input_ids'][0])))\n",
    "print('\\n\\ninputs 2 (after):\\n','|'.join(tokenizer.convert_ids_to_tokens(res['input_ids'][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.5455, 0.2727, 0.1818]\n",
    "\n",
    "# 15\n",
    "\n",
    "0.15 * (0.5455 + (0.2727 /2) + (0.1813/3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "BZ=32\n",
    "\n",
    "# data_collator_span_mlm =  DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "#                                               mlm=True,\n",
    "#                                               mlm_probability=0.15,\n",
    "#                                               max_gram=3,\n",
    "#                                               keep_prob=0.0,\n",
    "#                                               mask_prob=1.0,\n",
    "\n",
    "#                                               pad_to_multiple_of=8)\n",
    "\n",
    "# data_loader_span_mlm = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=BZ,\n",
    "#             sampler=train_sampler,\n",
    "#             collate_fn=data_collator_span_mlm,\n",
    "#             drop_last=False,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "\n",
    "# data_collator_subword_mlm =  DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "#                                               mlm=True,\n",
    "#                                               mlm_probability=0.15,\n",
    "#                                               pad_to_multiple_of=8)\n",
    "\n",
    "# data_loader_subword_mlm = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=BZ,\n",
    "#             sampler=train_sampler,\n",
    "#             collate_fn=data_collator_subword_mlm,\n",
    "#             drop_last=False,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#         )\n",
    "\n",
    "imp_data_collator_span_mlm =  ImprovedV2DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 ms ¬± 256 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-479-800325c8def5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"list(data_loader_subword_mlm)\\nprint('.', end='')\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/saiko/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             )\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/saiko/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mmask_tokens\u001b[0;34m(self, inputs, special_tokens_mask)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mprobability_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlm_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspecial_tokens_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             special_tokens_mask = [\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "list(data_loader_subword_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%prun next(iter(data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         334 function calls (331 primitive calls) in 0.004 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "       12    0.001    0.000    0.001    0.000 tensor.py:25(wrapped)\n",
       "        1    0.001    0.001    0.003    0.003 <ipython-input-807-b34b762e1e90>:66(mask_tokens)\n",
       "        1    0.000    0.000    0.001    0.001 {built-in method builtins.sum}\n",
       "        1    0.000    0.000    0.000    0.000 data_collator.py:195(_collate_batch)\n",
       "        3    0.000    0.000    0.001    0.000 <ipython-input-807-b34b762e1e90>:50(filter_indices)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method index_select}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method bernoulli}\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method full}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'nonzero' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.004    0.004 <string>:1(<module>)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method cat}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.003    0.003 data_collator.py:339(__call__)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method rsub}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'index_put_' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.004    0.004 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method min}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method tensor}\n",
       "       35    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "    40/39    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-807-b34b762e1e90>:73(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "       33    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "       32    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "        2    0.000    0.000    0.000    0.000 tensor.py:402(__rsub__)\n",
       "        1    0.000    0.000    0.004    0.004 dataloader.py:383(_next_data)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "       44    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        1    0.000    0.000    0.004    0.004 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1133(mask_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 tensor.py:449(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.003    0.003 fetch.py:42(fetch)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      2/1    0.000    0.000    0.004    0.004 {built-in method builtins.next}\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-807-b34b762e1e90>:127(<lambda>)\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-807-b34b762e1e90>:128(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "      3/2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-807-b34b762e1e90>:78(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(imp_data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         1873 function calls (1870 primitive calls) in 0.005 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "       32    0.001    0.000    0.001    0.000 tokenization_utils_base.py:3128(<listcomp>)\n",
       "        1    0.000    0.000    0.004    0.004 data_collator.py:361(mask_tokens)\n",
       "        1    0.000    0.000    0.001    0.001 data_collator.py:195(_collate_batch)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method bernoulli}\n",
       "       32    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1225(all_special_tokens_extended)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'new_full' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'random_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method tensor}\n",
       "       34    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:220(convert_tokens_to_ids)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method randint}\n",
       "       32    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1198(special_tokens_map_extended)\n",
       "      258    0.000    0.000    0.000    0.000 {method 'token_to_id' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'masked_fill_' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C._TensorBase' objects}\n",
       "       32    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1222(<listcomp>)\n",
       "      258    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:242(_convert_token_to_id_with_added_voc)\n",
       "      256    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        1    0.000    0.000    0.002    0.002 data_collator.py:372(<listcomp>)\n",
       "       32    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
       "      292    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:320(__init__)\n",
       "       32    0.000    0.000    0.001    0.000 tokenization_utils_base.py:1215(all_special_tokens)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method full}\n",
       "        1    0.000    0.000    0.005    0.005 {built-in method builtins.exec}\n",
       "      288    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'bool' of 'torch._C._TensorBase' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method empty}\n",
       "       32    0.000    0.000    0.002    0.000 tokenization_utils_base.py:3101(get_special_tokens_mask)\n",
       "        1    0.000    0.000    0.004    0.004 data_collator.py:339(__call__)\n",
       "       32    0.000    0.000    0.001    0.000 tokenization_utils_base.py:1241(all_special_ids)\n",
       "       35    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 sampler.py:198(__iter__)\n",
       "        1    0.000    0.000    0.004    0.004 dataloader.py:344(__next__)\n",
       "        1    0.000    0.000    0.005    0.005 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:44(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:375(__init__)\n",
       "       33    0.000    0.000    0.000    0.000 data_collator.py:215(<genexpr>)\n",
       "    36/35    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.004    0.004 dataloader.py:383(_next_data)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'get_vocab_size' of 'tokenizers.Tokenizer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 sampler.py:61(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1010(mask_token)\n",
       "       32    0.000    0.000    0.000    0.000 datasets.py:130(__getitem__)\n",
       "      3/2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:36(create_fetcher)\n",
       "        1    0.000    0.000    0.004    0.004 fetch.py:42(fetch)\n",
       "        1    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_fast.py:153(__len__)\n",
       "       32    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:275(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:338(_next_index)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1104(pad_token_id)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "      2/1    0.000    0.000    0.004    0.004 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:33(is_available)\n",
       "        2    0.000    0.000    0.000    0.000 dataloader.py:281(_auto_collation)\n",
       "        1    0.000    0.000    0.000    0.000 fetch.py:39(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 data_collator.py:203(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 tokenization_utils_base.py:989(pad_token)\n",
       "        1    0.000    0.000    0.000    0.000 datasets.py:127(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.000    0.000 dataloader.py:285(_index_sampler)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun next(iter(data_loader_subword_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 ms ¬± 51.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(iter(imp_data_loader_span_mlm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(imp_data_loader_span_mlm)\n",
    "print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = imp_data_collator_span_mlm((inputs_1, inputs_2))\n",
    "print(res)\n",
    "\n",
    "print(sum(torch.sum(res['input_ids'].eq(1), dim=1).detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14], [81])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_mask([res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 48])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['input_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assurance ü•Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mask(results):\n",
    "    mask_counts = []\n",
    "    token_counts = []\n",
    "    mask_hits = []\n",
    "    for item in results:\n",
    "#         print(item['labels'])\n",
    "        mask_count = sum(torch.sum(~(item['labels'].eq(-100)), dim=1).detach().cpu().numpy())\n",
    "    \n",
    "    \n",
    "        mask_hit = (item['labels'] != -100).nonzero(as_tuple=False).detach().cpu().numpy().tolist()    \n",
    "                    \n",
    "        special_tokens_count = sum(torch.sum(( item['input_ids'].eq(1) | item['input_ids'].eq(5)  |item['input_ids'].eq(6) ), dim=1).detach().cpu().numpy())\n",
    "\n",
    "        _token_count = item['input_ids'].shape[0] * item['input_ids'].shape[1]\n",
    "#         print('_token_count', _token_count)\n",
    "        token_count = _token_count - special_tokens_count\n",
    "#         print('token_count', token_count)\n",
    "        token_counts.append(token_count)\n",
    "\n",
    "        mask_hits.extend(mask_hit)\n",
    "        mask_counts.append(mask_count)\n",
    "    return mask_counts, token_counts, mask_hits\n",
    "\n",
    "\n",
    "def run_exp_masking_percentage(data_loader):\n",
    "    \n",
    "    result = list(data_loader)\n",
    "    \n",
    "    mask_counts, token_counts, mask_hits = count_mask(result)\n",
    "#     print('mask_counts', mask_counts)\n",
    "#     print('token_counts', token_counts)\n",
    "#     print('mask_hits', mask_hits)\n",
    "    total_mask_tokens = sum(mask_counts)\n",
    "    total_tokens = sum(token_counts)\n",
    "    percentage = total_mask_tokens / total_tokens * 100\n",
    "    print(f'total_mask_tokens: {total_mask_tokens}')\n",
    "    print(f'total_tokens: {total_tokens}')\n",
    "    print(f'masking percentage: {percentage:.3f}')\n",
    "    return percentage , mask_counts, token_counts, mask_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvals tensor([0.5455, 0.2727, 0.1818])\n",
      "pvals_adjusted tensor([0.5455, 0.6000, 1.0000])\n",
      "pvals tensor([0.5455, 0.2727, 0.1818])\n",
      "pvals_adjusted tensor([0.5455, 0.6000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "BZ=32\n",
    "imp_data_collator_span_mlm =  ImprovedV2DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=0.15,\n",
    "                                              max_gram=3,\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "exp 1\n",
      "total_mask_tokens: 314803\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.469\n",
      "\n",
      "\n",
      "exp 2\n",
      "total_mask_tokens: 315103\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.487\n",
      "\n",
      "\n",
      "exp 3\n",
      "total_mask_tokens: 315982\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.542\n",
      "\n",
      "\n",
      "exp 4\n",
      "total_mask_tokens: 315273\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.498\n",
      "\n",
      "\n",
      "exp 5\n",
      "total_mask_tokens: 315064\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.485\n",
      "\n",
      "\n",
      "exp 6\n",
      "total_mask_tokens: 315770\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.529\n",
      "\n",
      "\n",
      "exp 7\n",
      "total_mask_tokens: 316043\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.545\n",
      "\n",
      "\n",
      "exp 8\n",
      "total_mask_tokens: 314959\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.478\n",
      "\n",
      "\n",
      "exp 9\n",
      "total_mask_tokens: 314729\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.464\n",
      "\n",
      "\n",
      "exp 10\n",
      "total_mask_tokens: 314649\n",
      "total_tokens: 1616963\n",
      "masking percentage: 19.459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean     19.495653\n",
       "std       0.031985\n",
       "min      19.459258\n",
       "25%      19.471194\n",
       "50%      19.486129\n",
       "75%      19.520901\n",
       "max      19.545469\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = []\n",
    "for i in range(10):\n",
    "    print(f'\\n\\nexp {i+1}')\n",
    "    percentages.append(run_exp_masking_percentage(data_loader=imp_data_loader_span_mlm))\n",
    "    \n",
    "percentages_df = pd.Series(percentages)\n",
    "percentages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvals tensor([0.5455, 0.2727, 0.1818])\n",
      "pvals_adjusted tensor([0.5455, 0.6000, 1.0000])\n",
      "pvals tensor([0.4380, 0.2190, 0.1460, 0.1095, 0.0876])\n",
      "pvals_adjusted tensor([0.4380, 0.3896, 0.4255, 0.5556, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "BZ = 32\n",
    "imp_data_collator_span_mlm =  ImprovedV2DataCollatorForSpanLevelMask(tokenizer=tokenizer,\n",
    "                                              mlm=True,\n",
    "                                              mlm_probability=.2,\n",
    "                                              max_gram=5,\n",
    "\n",
    "                                              pad_to_multiple_of=8)\n",
    "\n",
    "imp_data_loader_span_mlm_large = DataLoader(\n",
    "            train_dataset_large,\n",
    "            batch_size=BZ,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=imp_data_collator_span_mlm,\n",
    "            drop_last=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "exp 1\n",
      "total_mask_tokens: 214482\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.802\n",
      "\n",
      "\n",
      "exp 2\n",
      "total_mask_tokens: 214163\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.770\n",
      "\n",
      "\n",
      "exp 3\n",
      "total_mask_tokens: 214427\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.797\n",
      "\n",
      "\n",
      "exp 4\n",
      "total_mask_tokens: 214047\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.758\n",
      "\n",
      "\n",
      "exp 5\n",
      "total_mask_tokens: 214765\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.831\n",
      "\n",
      "\n",
      "exp 6\n",
      "total_mask_tokens: 216120\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.969\n",
      "\n",
      "\n",
      "exp 7\n",
      "total_mask_tokens: 215169\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.872\n",
      "\n",
      "\n",
      "exp 8\n",
      "total_mask_tokens: 213513\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.704\n",
      "\n",
      "\n",
      "exp 9\n",
      "total_mask_tokens: 214384\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.793\n",
      "\n",
      "\n",
      "exp 10\n",
      "total_mask_tokens: 214852\n",
      "total_tokens: 983751\n",
      "masking percentage: 21.840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean     21.813670\n",
       "std       0.071824\n",
       "min      21.703968\n",
       "25%      21.775658\n",
       "50%      21.799673\n",
       "75%      21.837869\n",
       "max      21.968974\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages = []\n",
    "for i in range(10):\n",
    "    print(f'\\n\\nexp {i+1}')\n",
    "    percentages.append(run_exp_masking_percentage(data_loader=imp_data_loader_span_mlm_large))\n",
    "    \n",
    "percentages_df = pd.Series(percentages)\n",
    "percentages_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
