{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from datasets import load_dataset, load_metric\n",
    "from thai2transformers.metrics import classification_metrics\n",
    "from pythainlp.ulmfit import process_thai\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "class Args:\n",
    "    dataset_name_or_path = 'prachathai67k'\n",
    "    feature_col = 'title'\n",
    "    label_cols = ['politics', 'human_rights', 'quality_of_life', \n",
    "                  'international', 'social', 'environment', 'economics', \n",
    "                  'culture', 'labor', 'national_security', 'ict', 'education']\n",
    "    metric_for_best_model = 'f1_macro'\n",
    "    seed = 1412\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset prachathai67k (/Users/admin/.cache/huggingface/datasets/prachathai67k/prachathai67k/1.1.0/2eeb3bfaf307043e606a58f1f2af8b3d6bbf8a2d0b957d7bfafaf1dc1ef4b5ac)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'date', 'title', 'body_text', 'politics', 'human_rights', 'quality_of_life', 'international', 'social', 'environment', 'economics', 'culture', 'labor', 'national_security', 'ict', 'education'],\n",
       "        num_rows: 54379\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['url', 'date', 'title', 'body_text', 'politics', 'human_rights', 'quality_of_life', 'international', 'social', 'environment', 'economics', 'culture', 'labor', 'national_security', 'ict', 'education'],\n",
       "        num_rows: 6721\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['url', 'date', 'title', 'body_text', 'politics', 'human_rights', 'quality_of_life', 'international', 'social', 'environment', 'economics', 'culture', 'labor', 'national_security', 'ict', 'education'],\n",
       "        num_rows: 6789\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(args.dataset_name_or_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbsvm class\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, penalty='l2', C=1.0, dual=False, seed=1412):\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.seed = seed\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.toarray().ravel() if type(y)!=np.ndarray else y.ravel()\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(penalty = self.penalty, \n",
    "                                       C=self.C, \n",
    "                                       dual=self.dual,\n",
    "                                       solver='liblinear',\n",
    "                                       random_state=self.seed,).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = dataset['train'][args.feature_col]\n",
    "texts_valid = dataset['validation'][args.feature_col]\n",
    "texts_test = dataset['test'][args.feature_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<54379x57108 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1275794 stored elements in Compressed Sparse Row format>,\n",
       " <6721x57108 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 152058 stored elements in Compressed Sparse Row format>,\n",
       " <6789x57108 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 152024 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), tokenizer=process_thai,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "x_train = tfidf.fit_transform(texts_train)\n",
    "x_valid = tfidf.transform(texts_valid)\n",
    "x_test = tfidf.transform(texts_test)\n",
    "x_train,x_valid,x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54379, 12), (6721, 12), (6789, 12))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y\n",
    "import numpy as np\n",
    "y_train = np.array([dataset['train'][col] for col in args.label_cols]).transpose()\n",
    "y_valid = np.array([dataset['validation'][col] for col in args.label_cols]).transpose()\n",
    "y_test = np.array([dataset['test'][col] for col in args.label_cols]).transpose()\n",
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholding\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def best_threshold(y, probs):\n",
    "    f1s = []\n",
    "    for th in range(1,100):\n",
    "        f1s.append((th/100,f1_score(y,(probs> (th/100)).astype(int))))\n",
    "    f1s_df = pd.DataFrame(f1s).sort_values(1,ascending=False).reset_index(drop=True)\n",
    "    f1s_df.columns = ['th_label','f1_label']\n",
    "    return f1s_df.th_label[0], f1s_df.f1_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.611050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.607425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.605610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.601663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.585137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.578731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.574738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  penalty  C  f1_macro\n",
       "0      l2  1  0.611050\n",
       "1      l2  2  0.607425\n",
       "2      l2  3  0.605610\n",
       "3      l2  4  0.601663\n",
       "4      l1  1  0.590170\n",
       "5      l1  2  0.585137\n",
       "6      l1  3  0.578731\n",
       "7      l1  4  0.574738"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validation\n",
    "hyperparams = []\n",
    "for p in ['l1','l2']:\n",
    "    for c in range(1,5):\n",
    "        d = {'penalty':p, 'C':c, 'seed': seed}\n",
    "        for i in range(y_valid.shape[1]):\n",
    "            if p == 'l1':\n",
    "                model = NbSvmClassifier(penalty='l1', \n",
    "                                        C=c, \n",
    "                                        dual=False,\n",
    "                                        seed=seed).fit(x_train, y_train[:,i])\n",
    "            else:\n",
    "                model = NbSvmClassifier(penalty='l2', \n",
    "                                        C=c, \n",
    "                                        dual=True,\n",
    "                                        seed=seed).fit(x_train, y_train[:,i])\n",
    "            probs = model.predict_proba(x_valid)[:,1]\n",
    "            d[f'th_label_{i}'], d[f'f1_label_{i}'] = best_threshold(y_valid[:,i],probs)\n",
    "        #micro f1\n",
    "        d['f1_macro'] = np.mean([d[f'f1_label_{i}'] for i in range(y_valid.shape[1])])\n",
    "        hyperparams.append(d)\n",
    "        \n",
    "hyperparams_df = pd.DataFrame(hyperparams).sort_values('f1_macro',ascending=False).reset_index(drop=True)\n",
    "best_hyperparams = hyperparams_df[['penalty','C','seed']+[f'th_label_{i}' for i in range(y_valid.shape[1])]].iloc[0,:].to_dict()\n",
    "hyperparams_df[['penalty','C','f1_macro']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "probs = np.zeros((x_test.shape[0], y_test.shape[1]))\n",
    "preds = np.zeros((x_test.shape[0], y_test.shape[1]))\n",
    "for i in range(y_test.shape[1]):\n",
    "    model = NbSvmClassifier(penalty=best_hyperparams['penalty'],\n",
    "                           C=best_hyperparams['C'],\n",
    "                           seed=best_hyperparams['seed']).fit(x_train, y_train[:,i])\n",
    "    probs[:,i] = model.predict_proba(x_test)[:,1]\n",
    "    preds[:,i] = (probs[:,i] > best_hyperparams[f'th_label_{i}']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6789, 12), (6789, 12), (6789, 12))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, preds.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#micro\n",
    "micro_df = pd.DataFrame.from_dict({'accuracy': (preds==y_test).mean(),\n",
    "'f1_micro':f1_score(y_test.reshape(-1),preds.reshape(-1)),\n",
    "'precision_micro':precision_score(y_test.reshape(-1),preds.reshape(-1)),\n",
    "'recall_micro':recall_score(y_test.reshape(-1),preds.reshape(-1))}, orient='index').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#macro\n",
    "test_performances = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    d = {}\n",
    "    d['f1_macro'] = f1_score(y_test[:,i],preds[:,i])\n",
    "    d['precision_macro'] = precision_score(y_test[:,i],preds[:,i])\n",
    "    d['recall_macro'] = recall_score(y_test[:,i],preds[:,i])\n",
    "    test_performances.append(d)\n",
    "macro_df = pd.DataFrame(pd.DataFrame(test_performances).mean()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.903913</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>0.627623</td>\n",
       "      <td>0.71334</td>\n",
       "      <td>0.607269</td>\n",
       "      <td>0.589908</td>\n",
       "      <td>0.633342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  f1_micro  precision_micro  recall_micro  f1_macro  \\\n",
       "0  0.903913  0.667742         0.627623       0.71334  0.607269   \n",
       "\n",
       "   precision_macro  recall_macro  \n",
       "0         0.589908      0.633342  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test performance\n",
    "test_df = pd.concat([micro_df,macro_df],1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
